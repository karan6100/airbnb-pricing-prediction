{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Modeling\n",
    "\n",
    "## Implement Baselines\n",
    "\n",
    "To fit our baseline model we will use OLS (Ordinary Least Squares Regression). We will split our dataset into a train and test (65 / 35) and run 10 Linear Regression simulations to calculate the Train and Test Score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>host_id</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>guests_included</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>host_listing_count</th>\n",
       "      <th>10001</th>\n",
       "      <th>10002</th>\n",
       "      <th>...</th>\n",
       "      <th>40-49</th>\n",
       "      <th>50-59</th>\n",
       "      <th>60-69</th>\n",
       "      <th>70-79</th>\n",
       "      <th>80-84</th>\n",
       "      <th>85-89</th>\n",
       "      <th>90-95</th>\n",
       "      <th>95-100</th>\n",
       "      <th>No Reviews</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1069266</td>\n",
       "      <td>5867023</td>\n",
       "      <td>-0.517323</td>\n",
       "      <td>-0.405960</td>\n",
       "      <td>-0.490869</td>\n",
       "      <td>0.500815</td>\n",
       "      <td>2.672420</td>\n",
       "      <td>-0.359693</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2061725</td>\n",
       "      <td>4601412</td>\n",
       "      <td>-0.517323</td>\n",
       "      <td>-0.405960</td>\n",
       "      <td>0.387294</td>\n",
       "      <td>-0.459368</td>\n",
       "      <td>1.269925</td>\n",
       "      <td>0.927756</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44974</td>\n",
       "      <td>198425</td>\n",
       "      <td>-0.517323</td>\n",
       "      <td>-0.405960</td>\n",
       "      <td>-0.490869</td>\n",
       "      <td>-0.459368</td>\n",
       "      <td>0.802427</td>\n",
       "      <td>-0.359693</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4701675</td>\n",
       "      <td>22590025</td>\n",
       "      <td>-0.517323</td>\n",
       "      <td>-0.405960</td>\n",
       "      <td>0.387294</td>\n",
       "      <td>-0.459368</td>\n",
       "      <td>-0.496178</td>\n",
       "      <td>-0.359693</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68914</td>\n",
       "      <td>343302</td>\n",
       "      <td>1.693096</td>\n",
       "      <td>1.271321</td>\n",
       "      <td>1.265456</td>\n",
       "      <td>0.500815</td>\n",
       "      <td>0.282985</td>\n",
       "      <td>0.069457</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   host_id  accommodates  bedrooms      beds  guests_included  \\\n",
       "0  1069266   5867023     -0.517323 -0.405960 -0.490869         0.500815   \n",
       "1  2061725   4601412     -0.517323 -0.405960  0.387294        -0.459368   \n",
       "2    44974    198425     -0.517323 -0.405960 -0.490869        -0.459368   \n",
       "3  4701675  22590025     -0.517323 -0.405960  0.387294        -0.459368   \n",
       "4    68914    343302      1.693096  1.271321  1.265456         0.500815   \n",
       "\n",
       "   number_of_reviews  host_listing_count  10001  10002  ...    40-49  50-59  \\\n",
       "0           2.672420           -0.359693      0      0  ...        0      0   \n",
       "1           1.269925            0.927756      0      0  ...        0      0   \n",
       "2           0.802427           -0.359693      0      0  ...        0      0   \n",
       "3          -0.496178           -0.359693      0      0  ...        0      0   \n",
       "4           0.282985            0.069457      0      0  ...        0      0   \n",
       "\n",
       "   60-69  70-79  80-84  85-89  90-95  95-100  No Reviews  price  \n",
       "0      0      0      0      1      0       0           0  160.0  \n",
       "1      0      0      0      0      0       1           0   58.0  \n",
       "2      0      0      0      0      0       1           0  185.0  \n",
       "3      0      0      0      0      0       1           0  195.0  \n",
       "4      0      0      0      0      0       1           0  165.0  \n",
       "\n",
       "[5 rows x 227 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../datasets/listings_clean.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into x and y (note that we do not include id and host_id as predictors)\n",
    "x = data.iloc[:, 2:-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35)\n",
    "    regression = Lin_Reg(fit_intercept=True)\n",
    "    regression.fit(x_train, y_train)\n",
    "    train_score = regression.score(x_train, y_train)\n",
    "    test_score = regression.score(x_test, y_test)\n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.315415815287\n",
      "Mean Test Score: -3.41570000691e+18\n"
     ]
    }
   ],
   "source": [
    "# 10 iterations\n",
    "training_scores = [None]*10\n",
    "testing_scores = [None]*10\n",
    "\n",
    "for i in range(10):\n",
    "    training_scores[i], testing_scores[i] = linear_regression(x, y)\n",
    "#     print 'Train Score {}:'.format(i+1), training_scores[i]\n",
    "#     print 'Test Score {}:'.format(i+1), testing_scores[i], '\\n'\n",
    "\n",
    "print 'Mean Train Score:', np.mean(training_scores)\n",
    "print 'Mean Test Score:', np.mean(testing_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that while we are achieving a low $R^2$ score on the train set, we are achieving an extremely negative $R^2$ score on the test set. Let's try a RidgeCV regression instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ridge regression: compute train and test score\n",
    "def ridge_regression(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35)\n",
    "    reg_params = 10.**np.linspace(-10, 5, 10)\n",
    "    ridge = RidgeCV(alphas=reg_params, fit_intercept=True, cv=5)\n",
    "    ridge.fit(x_train, y_train)\n",
    "    train_score = ridge.score(x_train, y_train)\n",
    "    test_score = ridge.score(x_test, y_test)\n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.298820752829\n",
      "Mean Test Score: 0.292137226259\n"
     ]
    }
   ],
   "source": [
    "# perform 10 iterations\n",
    "training_scores = [None]*10\n",
    "testing_scores = [None]*10\n",
    "\n",
    "# compute ridge regression train and test score\n",
    "for i in range(10):\n",
    "    training_scores[i], testing_scores[i] = ridge_regression(x, y)\n",
    "#     print 'Train Score {}:'.format(i+1), training_scores[i]\n",
    "#     print 'Test Score {}:'.format(i+1), testing_scores[i], '\\n'\n",
    "\n",
    "print 'Mean Train Score:', np.mean(training_scores)\n",
    "print 'Mean Test Score:', np.mean(testing_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lasso regression: compute train and test score\n",
    "def lasso_regression(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35)\n",
    "    reg_params = 10.**np.linspace(-10, 5, 10)\n",
    "    lasso = LassoCV(alphas=reg_params, fit_intercept=True, cv=5)\n",
    "    lasso.fit(x_train, y_train)\n",
    "    train_score = lasso.score(x_train, y_train)\n",
    "    test_score = lasso.score(x_test, y_test)\n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slamfyre175/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train Score: 0.298946032747\n",
      "Mean Test Score: 0.291913140191\n"
     ]
    }
   ],
   "source": [
    "# perform 10 iterations\n",
    "training_scores = [None]*10\n",
    "testing_scores = [None]*10\n",
    "\n",
    "# compute ridge regression train and test score\n",
    "for i in range(10):\n",
    "    training_scores[i], testing_scores[i] = lasso_regression(x, y)\n",
    "#     print 'Train Score {}:'.format(i+1), training_scores[i]\n",
    "#     print 'Test Score {}:'.format(i+1), testing_scores[i], '\\n'\n",
    "\n",
    "print 'Mean Train Score:', np.mean(training_scores)\n",
    "print 'Mean Test Score:', np.mean(testing_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using Ridge Regression as well as Lasso Regression has dramatically increased our $R^2$ for the test score. Both Regressions perform incredibly similar. We see that for both the $R^2$ for the train score in general remains the same. Let's try a RidgeCV polynomial next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for calculating BIC\n",
    "BIC = lambda n, d, RSS: n * np.log(RSS * 1. / n) + d * np.log(n)\n",
    "#Function for calculating Residual Sum of Squares\n",
    "RSS = lambda predict, actual: np.sum((predict - actual)**2)\n",
    "\n",
    "#Function for finding the best polynomial model using Ridge polynomial regression\n",
    "def best_poly_model(pred, response, max_deg, reg_params):\n",
    "    #Best regularization parameter for each degree\n",
    "    lambdas = []\n",
    "    #Best BIC for each degree\n",
    "    bics = []\n",
    "    #Best model parameters for each degree\n",
    "    params = []\n",
    "    \n",
    "    #Iterate through degrees 1 to max_degree\n",
    "    for degree in range(2, max_deg):\n",
    "        #Turn one predictor into 1, t, t^2, t^3, ...\n",
    "        poly_t = PolynomialFeatures(degree=degree)\n",
    "        pred_expanded = poly_t.fit_transform(pred)\n",
    "        \n",
    "        #Perform Ridge regression using expanded set of predictors, \n",
    "        #choose best regularization parameter lambda using 5-fold x-validation\n",
    "        ridge = RidgeCV(alphas=reg_params, fit_intercept=True, cv=5)\n",
    "        ridge.fit(pred_expanded, response)\n",
    "        \n",
    "        #Record the parameters of the model chosen by 5-fold x-validation\n",
    "        params.append(ridge.coef_)\n",
    "        #Record the lambda chosen by 5-fold x-validation\n",
    "        lambdas.append(ridge.alpha_)\n",
    "        \n",
    "        #Record the BIC score of the model chosen by 5-fold x-validation\n",
    "        response_hat = ridge.predict(pred_expanded)        \n",
    "        error = RSS(response_hat, response)\n",
    "        bics.append(BIC(pred.shape[0], degree, error))\n",
    "    \n",
    "    #Find the degree with the min BIC score\n",
    "    best_degree = np.argmin(bics) + 1\n",
    "    #Find the best lambda for the degree with the min BIC score\n",
    "    best_lambda = lambdas[best_degree - 1]\n",
    "    #Find the best model parameters for the degree with the min BIC score\n",
    "    best_params = params[best_degree - 1]\n",
    "    return best_degree, best_lambda, best_params       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------Warning---------\n",
    "\n",
    "This code takes forever to run. Would suspect the amount of observations is throwing off the polynomial prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slamfyre175/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/ridge.py:154: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    }
   ],
   "source": [
    "reg_params = 10.**np.linspace(-10, 5, 10)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35)\n",
    "best_degree, best_lambda = best_poly_model(x_train, y_train, 3, reg_params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print best_degree, best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_t = PolynomialFeatures(degree=best_degree)\n",
    "pred_expanded = poly_t.fit_transform(x_train)\n",
    "pred_test_expanded = poly_t.fit_transform(x_test)\n",
    "ridge = RidgeCV(alphas=[best_lambda], fit_intercept=True, cv=5)\n",
    "ridge.fit(pred_expanded, y_train)\n",
    "train_score = ridge.score(pred_expanded, y_train)\n",
    "test_score = ridge.score(pred_test_expanded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print train_score\n",
    "print test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest regressor\n",
    "def random_forest_model(x_train, y_train, x_test, y_test):\n",
    "#     best_num_p = 0\n",
    "#     best_num_tree = 0\n",
    "#     best_score = 0\n",
    "#     best_train_score = 0\n",
    "    # tune for parameter\n",
    "#     for i in range(1, len(x_train.columns)):\n",
    "#         # tune for tree depth from 1 to 20\n",
    "#         for j in range(1, 21):\n",
    "#             for k in range(1, 101, 20):\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(x_train, y_train)\n",
    "    score_train = rf.score(x_train, y_train)\n",
    "    score = rf.score(x_test, y_test)\n",
    "#     if score > best_score:\n",
    "#         best_score = score\n",
    "#         best_train_score = score_train\n",
    "#         best_num_p = i\n",
    "#         best_num_tree = j\n",
    "    return score_train, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.35)\n",
    "rf_values = random_forest_model(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.770322616846\n",
      "Test Score: 0.213976690139\n"
     ]
    }
   ],
   "source": [
    "print 'Train Score:', rf_values[0]\n",
    "print 'Test Score:', rf_values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a significantly higher train score for Random Forest Regressor but a lower Test Score. This is certainly promising. With further tuning, we can expect the test score to increase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
